# -*- coding: utf-8 -*-
"""test_Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mSg9fBvXqb9AIXDfp3nKEKXAZejEoQyX
"""

!nvidia-smi

from google.colab import drive
drive.mount("/content/drive")

!pip install python-dotenv

import pandas as pd
import os
import csv
import torch
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import f1_score
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    TrainingArguments, Trainer, TrainerCallback, EvalPrediction
    )
import ast

# 讀取csv檔案，並將 new_label 欄位從字串轉換為真正的 Python list
# 使用 ast.literal_eval 可安全解析字串格式的 list
grouped = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/test/merged_data_0517.csv", converters={"new_label": ast.literal_eval})

print(grouped.head())
print(grouped['new_label'].shape)

mlb = MultiLabelBinarizer()
y = mlb.fit_transform(grouped['new_label'])
print(y.shape)

print(type(grouped["new_label"].iloc[0]))

train_texts, val_texts, train_labels, val_labels = train_test_split(
    grouped['text'], y, test_size=0.2, random_state=42
)

tokenizer = BertTokenizer.from_pretrained("hfl/chinese-roberta-wwm-ext")

# ===== Dataset 定義 =====

class MultiLabelDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(
            list(tqdm(texts, desc="Tokenizing")),
            truncation=True, padding=True, max_length=512
        )
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = MultiLabelDataset(train_texts, train_labels)
val_dataset = MultiLabelDataset(val_texts, val_labels)

# ===== 模型載入 =====

model = BertForSequenceClassification.from_pretrained(
    "hfl/chinese-roberta-wwm-ext",
    num_labels=y.shape[1],
    problem_type="multi_label_classification"
)

# ===== 評估指標 =====

def compute_metrics(pred: EvalPrediction):
    logits, labels = pred
    #確保 labels 是 numpy array
    labels = labels if isinstance(labels, np.ndarray) else labels.numpy()
    labels = labels.astype(int)
    #將 logits 經 sigmoid 轉成機率（適用於多標籤分類）
    preds = torch.sigmoid(torch.tensor(logits)).numpy()
    #將機率大於等於 0.5 的預測視為正類（1），否則為負類（0）
    preds_binary = (preds >= 0.5).astype(int)

    micro_f1 = f1_score(labels, preds_binary, average='micro')
    macro_f1 = f1_score(labels, preds_binary, average='macro')

    ## --- P.S. ---
    #每個標籤預測錯的平均比例，越低越好。
    hamming = hamming_loss(labels, preds_binary)
    # --- P.S. --- ##

    #完全正確才算對的準確率（所有標籤都預測正確才算對）
    subset_accuracy = (preds_binary == labels).all(axis=1).mean()
    #每個樣本的 Jaccard 相似度（交集/聯集），再取平均
    sample_accuracy = (preds_binary & labels).sum(axis=1) / (
        (preds_binary | labels).sum(axis=1) + 1e-8
    )
    sample_accuracy = sample_accuracy.mean()

    return {
        "micro_f1": round(micro_f1, 6),
        "macro_f1": round(macro_f1, 6),
        "hamming_loss": round(hamming, 6),
        "subset_accuracy": round(subset_accuracy, 6),
        "sample_accuracy": round(sample_accuracy, 6)
    }

# ===== 自訂進度條 Callback =====

class TQDMProgressBar(TrainerCallback):
    def on_train_begin(self, args, state, control, **kwargs):
        self.train_bar = tqdm(total=state.max_steps, desc="Training")

    def on_step_end(self, args, state, control, **kwargs):
        self.train_bar.update(1)
        if 'logs' in kwargs and 'loss' in kwargs['logs']:
            self.train_bar.set_postfix(loss=kwargs["logs"]["loss"])

    def on_train_end(self, args, state, control, **kwargs):
        self.train_bar.close()

# ===== 訓練參數設定 =====

output_dir = "/content/drive/MyDrive/Colab Notebooks/test/model_output10"
os.makedirs(output_dir, exist_ok=True)

training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    eval_strategy="epoch", # Changed from evaluation_strategy to eval_strategy
    save_strategy="epoch",
    save_total_limit=1,
    load_best_model_at_end=True,
    metric_for_best_model="micro_f1",
    greater_is_better=True,
    logging_dir=None,
    report_to=[]
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if torch.cuda.is_available():
  print('GPU available')
else:
  print('Please set GPU via Edit -> Notebook Settings.')

model.to(device)

# ===== Trainer 初始化與訓練 =====

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[TQDMProgressBar]
)

print("開始訓練...")
trainer.train()

# presist model path
trainer.save_model("/content/drive/MyDrive/Colab Notebooks/test/model_saved_0517")

# ===== 使用最佳模型進行預測並儲存 CSV =====

print("使用最佳模型進行評估與預測...")
predictions = trainer.predict(val_dataset)
logits = predictions.predictions
labels = predictions.label_ids
print(logits.shape, labels.shape)

probs = torch.sigmoid(torch.tensor(logits)).numpy()
preds_binary = (probs >= 0.5).astype(int)
val_text_list = list(val_texts)  # 確保順序正確

output_path = "/content/drive/MyDrive/Colab Notebooks/test/predictions_0517.csv"
with open(output_path, "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f, quotechar='"', quoting=csv.QUOTE_MINIMAL)
    header = ["索取資料題目", "預測承辦機關", "實際承辦機關"] + list(mlb.classes_)
    writer.writerow(header)

    for text, prob, true_labels in zip(val_text_list, probs, labels):
        predicted_labels = [mlb.classes_[i] for i, p in enumerate(prob) if p >= 0.5]
        true_label_names = [mlb.classes_[i] for i in range(len(true_labels)) if true_labels[i] == 1]
        if not predicted_labels:
            predicted_labels = ["其他"]
        if not true_label_names:
            true_label_names = ["其他"]

        writer.writerow([text, ";".join(predicted_labels), ";".join(true_label_names)] + list(prob))

print(f"預測結果已儲存至 {output_path}")

"""**Pipeline Prediction**"""

from transformers import TextClassificationPipeline

# load pretrained model and tokenizer

persist_model_path = "/content/drive/MyDrive/Colab Notebooks/test/model_saved_0517"
model = BertForSequenceClassification.from_pretrained(f"{persist_model_path}")
tokenizer = BertTokenizer.from_pretrained("hfl/chinese-roberta-wwm-ext")

pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)

while True:
    # request a console input the replace the hardcoded text
    print("請輸入要分類的索取資料題目:")
    user_input = input().strip()

    if not user_input:
        print("請輸入有效的索取資料題目")
        exit(1)

    predict_result = pipe(user_input)
    # find the top 3 scores in predict_result[0]
    top_three_score_result = sorted(predict_result[0], key=lambda x: x['score'], reverse=True)[:3]
    # convert the label to the original valid_labels
    top_three_score_result = [
        {
            "label": label['label'],
            "score": label['score'],
            "label_name": mlb.classes_[int(label['label'].replace("LABEL_", ""))] if label['label'].startswith("LABEL_") else label['label']
        }
        for label in top_three_score_result
    ]

    # print the top 3 scores and label_name
    for label in top_three_score_result:
        print(f"Label: {label['label_name']}, Score: {label['score']:.4f}")

# print(top_three_score_result)